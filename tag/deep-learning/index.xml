<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Hefei Mei</title>
    <link>https://example.com/tag/deep-learning/</link>
      <atom:link href="https://example.com/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 26 Jun 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_huf2c2e11aa87fd12d9052bfa85f3feddc_130472_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://example.com/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Visual-based 3D Space Semantic Perception System</title>
      <link>https://example.com/project/yoloslam/</link>
      <pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/yoloslam/</guid>
      <description>&lt;p&gt;We choose the domestic Feiteng FT-2000 development board, which is equipped with the Galaxy Kylin system, to solve the problem of processing platform. We chose Yolo Fastest V2 as the object detection algorithm and used SVO for depth map construction. By fusing key points and detection box information, a three-dimensional spatial point cloud map with semantic information is obtained.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research on the Interpretability of Object Detection Networks</title>
      <link>https://example.com/project/interpretable/</link>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/interpretable/</guid>
      <description>&lt;p&gt;This paper completes the interpretative model of Score-CAM method on the Fast R-CNN network, and outputs the saliency map of the high impact area for each detection frame of the network. By observing the distribution of saliency degree, this paper proposes that the high saliency degree of detection frame contains both decision score information and frame location information. By detecting the outward extension of the detection frame when the objects are adjacent to each other, the relationship between the frame location and the high saliency degree inside the frame is verified by the way of frame shrinkage, which provides a theoretical basis for the optimization of the detection location using the class activation saliency map.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research on Visual Object Tracking Technology</title>
      <link>https://example.com/project/tracking/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/tracking/</guid>
      <description>&lt;p&gt;This project focuses on target tracking under the Siamese network framework, and achieves high-precision target tracking algorithms in complex backgrounds by optimizing the network. The current project mainly focuses on the implementation of target tracking algorithms based on the deep twin network SiamFC. Relevant filtering is added to the pre trained CNN features, combining the excellent offline discrimination ability of CNN with the effective online learning ability of CF to complete the implementation of CFNet network.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
